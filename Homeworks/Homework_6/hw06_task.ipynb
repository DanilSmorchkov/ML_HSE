{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "drYEhVJcLLS3"
   },
   "source": [
    "# This cat does not exist\n",
    "__Суммарное количество баллов: 10__\n",
    "\n",
    "Цель этого задания - создать котов, которых не существует. В ходе данного задания вы обучите DCGAN и VAE, которые являются одними из первых генеративных моделей. Для этого задания вам наверняка потребуется GPU с CUDA, поэтому рекомендуется использовать Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "ivL-X-OhLLS6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os \n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "jFxnL6uhLLS8",
    "ExecuteTime": {
     "end_time": "2024-02-12T07:30:50.750983500Z",
     "start_time": "2024-02-12T07:30:50.732572500Z"
    }
   },
   "outputs": [],
   "source": [
    "def random_noise(batch_size, channels, side_size):\n",
    "    return torch.randn(batch_size, channels, side_size, side_size).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "j47U0kZGLLS8",
    "ExecuteTime": {
     "end_time": "2024-02-12T07:30:59.947293400Z",
     "start_time": "2024-02-12T07:30:59.872178100Z"
    }
   },
   "outputs": [],
   "source": [
    "def visualise(imgs, rows=2):\n",
    "    imgs = (imgs.transpose(1, 3) + 1) / 2\n",
    "    imgs = torch.cat([imgs[i::rows] for i in range(rows)], dim=1)\n",
    "    cols = len(imgs)\n",
    "    imgs = (torch.cat(list(imgs), dim=1)).cpu().numpy()[:, :, ::-1]\n",
    "    plt.figure(figsize=(cols*1.5, rows*1.5))\n",
    "    plt.imshow(imgs)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "KQ2thNUcLLS9",
    "ExecuteTime": {
     "end_time": "2024-02-12T07:31:01.826936900Z",
     "start_time": "2024-02-12T07:31:00.597716300Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mCatDataset\u001B[39;00m(\u001B[43mDataset\u001B[49m):\n\u001B[0;32m      2\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, path_to_dataset\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcat_136\u001B[39m\u001B[38;5;124m\"\u001B[39m, size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m64\u001B[39m):\n\u001B[0;32m      3\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mphoto_names \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mlistdir(path_to_dataset)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'Dataset' is not defined"
     ]
    }
   ],
   "source": [
    "class CatDataset(Dataset):\n",
    "    def __init__(self, path_to_dataset=\"cat_136\", size=64):\n",
    "        self.photo_names = os.listdir(path_to_dataset)\n",
    "        self.path_base = path_to_dataset\n",
    "        self.size = size\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        path = self.path_base + \"/\" + self.photo_names[index]\n",
    "        img = cv2.imread(path) # 136 x 136\n",
    "        crop_rate = 8\n",
    "        x_crop = random.randint(0, crop_rate)\n",
    "        y_crop = random.randint(0, crop_rate)\n",
    "        img = img[x_crop:136 - crop_rate + x_crop, y_crop:136 - crop_rate + y_crop]\n",
    "        img = cv2.resize(img, (self.size, self.size), interpolation=cv2.INTER_CUBIC)\n",
    "        return 2 * torch.tensor(img).float().transpose(0, 2) / 255. - 1\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.photo_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "vpjrw6A3LLS9",
    "ExecuteTime": {
     "end_time": "2024-02-12T07:31:01.978012Z",
     "start_time": "2024-02-12T07:31:01.865570300Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CatDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m dataset \u001B[38;5;241m=\u001B[39m \u001B[43mCatDataset\u001B[49m()\n\u001B[0;32m      2\u001B[0m visualise(torch\u001B[38;5;241m.\u001B[39mcat([dataset[i]\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m15\u001B[39m, \u001B[38;5;241m182\u001B[39m, \u001B[38;5;241m592\u001B[39m, \u001B[38;5;241m394\u001B[39m, \u001B[38;5;241m2941\u001B[39m]], dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m))\n",
      "\u001B[1;31mNameError\u001B[0m: name 'CatDataset' is not defined"
     ]
    }
   ],
   "source": [
    "dataset = CatDataset()\n",
    "visualise(torch.cat([dataset[i].unsqueeze(0) for i in [3, 15, 182, 592, 394, 2941]], dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nXv8hAXULLS-"
   },
   "source": [
    "### Задание 1 (2 балла)\n",
    "Для начала реализуем кодировщик для нашего VAE. Предлагается использовать следующую архитектуру:\n",
    "\n",
    "![](imgs/VAE_enc.png)\n",
    "\n",
    "Для ее реализации вам потребуются модули `nn.Conv2d`, `nn.ReLU`, `nn.Flatten` и `nn.Linear`.\n",
    "\n",
    "#### Методы\n",
    "* `__init__` - принимает на вход `img_size`, `latent_size`, `start_channels` и `downsamplings`. Первый аргумент - высота и ширина картинки в пикселях. Второй аргумент - размерность латентного пространства. `start_channels` отвечает за то, сколько каналов должно быть в картинке перед тем, как к ней будут применены downsampling блоки. `downsamplings` - это количество downsampling блоков, которые должны быть применены к картинке. В каждом таком блоке количество каналов увеличивается в два раза.\n",
    "\n",
    "\n",
    "* `forward` - принимает на вход батч `x`, возвращает эмбеддинг в латентном пространстве `z` и параметры распределения `(mu, sigma)`.\n",
    "\n",
    "\n",
    "#### Важно\n",
    "`Linear Model` в схеме - это полносвязная сеть из нескольких слоев. Предлагается такая архитектура:\n",
    "\n",
    "`[Linear(c * img_size // 2^N -> 256), ReLU, Linear(256 -> 2 * latent_size)]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "oG1-PV6oLLS_"
   },
   "outputs": [],
   "source": [
    "from task import Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E0UM3HHpLLS_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8up80_Z-LLTA"
   },
   "source": [
    "### Задание 2 (3 балла)\n",
    "Теперь реализуем декодер для VAE. Предлагается использовать следующую архитектуру:\n",
    "\n",
    "![](imgs/VAE_dec.png)\n",
    "\n",
    "Для ее реализации вам потребуются модули `nn.Linear`, `nn.Unflatten`, `nn.ConvTranspose2d`, `nn.ReLU` и `nn.Tanh`.\n",
    "\n",
    "#### Методы\n",
    "* `__init__` - принимает на вход `img_size`, `latent_size`, `end_channels` и `upsamplings`.  Первый аргумент - высота и ширина картинки в пикселях. Второй аргумент - размерность латентного пространства. `end_channels` отвечает за то, сколько каналов должно быть после всех upsampling блоков. `upsamplings` - это количество upsampling блоков, которые должны быть применены к картинке. В каждом таком блоке количество каналов уменьшается в два раза.\n",
    "\n",
    "\n",
    "* `forward` - принимает на вход `z` - тензор с латентным представлением изображений. Возвращает батч восстановленных изображений.\n",
    "\n",
    "#### Важно\n",
    "`Linear Model` в схеме - это полносвязная сеть из нескольких слоев. Предлагается такая архитектура:\n",
    "\n",
    "`[Linear(latent_size -> 256), ReLU, Linear(256 -> c * 2^N)]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "a7I_Ry3CLLTB"
   },
   "outputs": [],
   "source": [
    "from task import Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WeFjX9cwLLTB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Huqptx5LLTC",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46M_oEWULLTC"
   },
   "source": [
    "### Задание 3 (5 баллов)\n",
    "Наконец, объединим Encoder и Decoder в Variational Autoencoder.\n",
    "\n",
    "Полностью архитектура выглядит так:\n",
    "![](imgs/VAE.png) \n",
    "\n",
    "Из нее можно выделить две части: Encoder (по изображению возвращает mu и sigma) и Decoder (по случайному шуму восстанавливает изображение). На высоком уровне VAE можно представить так:\n",
    "\n",
    "![](imgs/VAE_highlevel.png)\n",
    "\n",
    "В данном задании вам необходимо реализовать полную архитектуру VAE.\n",
    "\n",
    "#### Методы\n",
    "* `__init__` - принимает на вход `img_size`, `downsamplings`, `latent_size`, `linear_hidden_size`, `down_channels` и `up_channels`. `img_size` - размер стороны входного изображения. `downsamplings` - количество downsampling (и upsampling) блоков. `latent_size` - размер латентного пространства, в котором в который будет закодирована картинка. `linear_hidden_size` количество нейронов на скрытом слое полносвязной сети в конце encoder'а. Для полносвязной сети decoder'а это число стоит умножить на 2. `down_channels` - количество каналов, в которое будет преобразовано трехцветное изображение перед применением `downsampling` блоков. `up_channels` - количество каналов, которое должно получиться после применения всех upsampling блоков.\n",
    "\n",
    "* `forward` - принимает на вход `x`. Считает распределение $N(\\mu, \\sigma^2)$ и вектор $z \\sim N(\\mu, \\sigma^2)$. Возвращает $x'$ - восстановленную из вектора $z$ картинку и $D_{KL}(N(\\mu, \\sigma^2), N(0, 1)) = 0.5 \\cdot (\\sigma^2 + \\mu^2 - \\log \\sigma^2 - 1)$.\n",
    "\n",
    "* `encode` - принимает на вход `x`. Возвращает вектор из распределения $N(\\mu, \\sigma^2)$.\n",
    "\n",
    "* `decode` - принимает на вход `z`. Возвращает восстановленную по вектору картинку.\n",
    "\n",
    "* `save` - сохраняет чекпоинт обученной модели.\n",
    "\n",
    "* `load` - загружает сохраненный чекпоинт обученной модели.\n",
    "\n",
    "\n",
    "#### Если хочется улучшить качество\n",
    "https://arxiv.org/pdf/1906.00446.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "xtW6VpF4LLTC"
   },
   "outputs": [],
   "source": [
    "from task import VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2SXjsy_qLLTD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "DDfUzcbGLLTD",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[48], line 52\u001B[0m\n\u001B[1;32m     48\u001B[0m             rec_loss_avg \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m rec_loss\u001B[38;5;241m.\u001B[39mitem()\n\u001B[1;32m     50\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mep\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m | Reconstruction loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrec_loss_avg\u001B[38;5;250m \u001B[39m\u001B[38;5;241m/\u001B[39m\u001B[38;5;250m \u001B[39mtotal_batches\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m | KLD loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkld_loss_avg\u001B[38;5;250m \u001B[39m\u001B[38;5;241m/\u001B[39m\u001B[38;5;250m \u001B[39mtotal_batches\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 52\u001B[0m \u001B[43mtrain_vae\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[48], line 3\u001B[0m, in \u001B[0;36mtrain_vae\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtrain_vae\u001B[39m():\n\u001B[1;32m      2\u001B[0m     vae \u001B[38;5;241m=\u001B[39m VAE()\n\u001B[0;32m----> 3\u001B[0m     \u001B[43mvae\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcuda\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m     epochs \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m201\u001B[39m\n\u001B[1;32m      6\u001B[0m     batch_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m8\u001B[39m\n",
      "File \u001B[0;32m~/ML_HSE/ml_env/lib/python3.11/site-packages/torch/nn/modules/module.py:918\u001B[0m, in \u001B[0;36mModule.cuda\u001B[0;34m(self, device)\u001B[0m\n\u001B[1;32m    901\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcuda\u001B[39m(\u001B[38;5;28mself\u001B[39m: T, device: Optional[Union[\u001B[38;5;28mint\u001B[39m, device]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m T:\n\u001B[1;32m    902\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Moves all model parameters and buffers to the GPU.\u001B[39;00m\n\u001B[1;32m    903\u001B[0m \n\u001B[1;32m    904\u001B[0m \u001B[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    916\u001B[0m \u001B[38;5;124;03m        Module: self\u001B[39;00m\n\u001B[1;32m    917\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 918\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcuda\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/ML_HSE/ml_env/lib/python3.11/site-packages/torch/nn/modules/module.py:810\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn, recurse)\u001B[0m\n\u001B[1;32m    808\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[1;32m    809\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[0;32m--> 810\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    812\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[1;32m    813\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[1;32m    814\u001B[0m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[1;32m    815\u001B[0m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    820\u001B[0m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[1;32m    821\u001B[0m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[0;32m~/ML_HSE/ml_env/lib/python3.11/site-packages/torch/nn/modules/module.py:810\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn, recurse)\u001B[0m\n\u001B[1;32m    808\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[1;32m    809\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[0;32m--> 810\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    812\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[1;32m    813\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[1;32m    814\u001B[0m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[1;32m    815\u001B[0m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    820\u001B[0m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[1;32m    821\u001B[0m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[0;32m~/ML_HSE/ml_env/lib/python3.11/site-packages/torch/nn/modules/module.py:833\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn, recurse)\u001B[0m\n\u001B[1;32m    829\u001B[0m \u001B[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001B[39;00m\n\u001B[1;32m    830\u001B[0m \u001B[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001B[39;00m\n\u001B[1;32m    831\u001B[0m \u001B[38;5;66;03m# `with torch.no_grad():`\u001B[39;00m\n\u001B[1;32m    832\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m--> 833\u001B[0m     param_applied \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparam\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    834\u001B[0m should_use_set_data \u001B[38;5;241m=\u001B[39m compute_should_use_set_data(param, param_applied)\n\u001B[1;32m    835\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m should_use_set_data:\n",
      "File \u001B[0;32m~/ML_HSE/ml_env/lib/python3.11/site-packages/torch/nn/modules/module.py:918\u001B[0m, in \u001B[0;36mModule.cuda.<locals>.<lambda>\u001B[0;34m(t)\u001B[0m\n\u001B[1;32m    901\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcuda\u001B[39m(\u001B[38;5;28mself\u001B[39m: T, device: Optional[Union[\u001B[38;5;28mint\u001B[39m, device]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m T:\n\u001B[1;32m    902\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Moves all model parameters and buffers to the GPU.\u001B[39;00m\n\u001B[1;32m    903\u001B[0m \n\u001B[1;32m    904\u001B[0m \u001B[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    916\u001B[0m \u001B[38;5;124;03m        Module: self\u001B[39;00m\n\u001B[1;32m    917\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 918\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_apply(\u001B[38;5;28;01mlambda\u001B[39;00m t: \u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcuda\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m~/ML_HSE/ml_env/lib/python3.11/site-packages/torch/cuda/__init__.py:298\u001B[0m, in \u001B[0;36m_lazy_init\u001B[0;34m()\u001B[0m\n\u001B[1;32m    296\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCUDA_MODULE_LOADING\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m os\u001B[38;5;241m.\u001B[39menviron:\n\u001B[1;32m    297\u001B[0m     os\u001B[38;5;241m.\u001B[39menviron[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCUDA_MODULE_LOADING\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLAZY\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m--> 298\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_C\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_cuda_init\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    299\u001B[0m \u001B[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001B[39;00m\n\u001B[1;32m    300\u001B[0m \u001B[38;5;66;03m# we need to just return without initializing in that case.\u001B[39;00m\n\u001B[1;32m    301\u001B[0m \u001B[38;5;66;03m# However, we must not let any *other* threads in!\u001B[39;00m\n\u001B[1;32m    302\u001B[0m _tls\u001B[38;5;241m.\u001B[39mis_initializing \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "def train_vae():\n",
    "    vae = VAE()\n",
    "    vae.cuda()\n",
    "\n",
    "    epochs = 201\n",
    "    batch_size = 8\n",
    "    vae_optim = Adam(vae.parameters(), lr=1e-4)\n",
    "\n",
    "    dataset = CatDataset(size=128)\n",
    "\n",
    "    test_imgs_1 = torch.cat([dataset[i].unsqueeze(0) for i in (0, 34, 76, 1509)])\n",
    "    test_imgs_2 = torch.cat([dataset[i].unsqueeze(0) for i in (734, 123, 512, 3634)])\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        dataloader = DataLoader(dataset, shuffle=True, batch_size=batch_size)\n",
    "        total_batches = 0\n",
    "        rec_loss_avg = 0\n",
    "        kld_loss_avg = 0\n",
    "\n",
    "        if ep % 10 == 0:\n",
    "            with torch.no_grad():\n",
    "                z_1 = vae.encode(test_imgs_1.cuda())\n",
    "                z_2 = vae.encode(test_imgs_2.cuda())\n",
    "                x_int = []\n",
    "                for i in range(9):\n",
    "                    z = (i * z_1 + (8 - i) * z_2) / 8\n",
    "                    x_int.append(vae.decode(z))\n",
    "                x_int = torch.cat(x_int)\n",
    "                visualise(x_int, rows=len(test_imgs_1))\n",
    "                z_rand = torch.randn_like(z_1)\n",
    "                x_int = vae.decode(z_rand)\n",
    "                visualise(x_int, rows=len(test_imgs_1)//2)\n",
    "\n",
    "        for i, batch in tqdm(enumerate(dataloader), total=(len(dataset) + batch_size) // batch_size):\n",
    "            if len(batch) < batch_size:\n",
    "                continue\n",
    "            total_batches += 1\n",
    "            x = batch.cuda()\n",
    "            x_rec, kld = vae(x)\n",
    "            img_elems = float(np.prod(list(batch.size())))\n",
    "            kld_loss = kld.sum() / batch_size\n",
    "            rec_loss = ((x_rec - x)**2).sum() / batch_size\n",
    "            loss = rec_loss + 0.1 * kld_loss # https://openreview.net/forum?id=Sy2fzU9gl\n",
    "            vae_optim.zero_grad()\n",
    "            loss.backward()\n",
    "            vae_optim.step()\n",
    "            kld_loss_avg += kld_loss.item()\n",
    "            rec_loss_avg += rec_loss.item()\n",
    "\n",
    "        print(f\"Epoch {ep+1} | Reconstruction loss: {rec_loss_avg / total_batches} | KLD loss: {kld_loss_avg / total_batches}\")\n",
    "\n",
    "train_vae()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hw06_task.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
